import Admonition from '@components/Admonition.astro'

# Concurrency vs Parallelism

Concurrency and parallelism both exist to tackle the same fundamental set of problems, even though they approach them differently.

## The Problems 

At a high level, they aim to solve:

1. Efficient Use of Resources. Modern systems have **multiple CPU cores** able of performing several operations simultaneously.

2. Responsiveness. Programs often need to:

- Handle multiple users
- React to events
- Continue doing useful work while waiting (e.g., network or disk I/O)

Concurrency allows a program to stay responsive by structuring work so that long or blocking operations don’t freeze the entire system.

3. Throughput. Both aim to increase how much work gets done over time:

- Concurrency improves throughput by **overlapping tasks**.
- Parallelism improves throughput by **executing tasks simultaneously**.
- The goal in both cases is to process more work in less time.

5. Decomposing Complex Problems

Large problems are often easier to reason about when split into smaller, independent pieces. Both models encourage breaking work into units that can be:

- Executed independently
- Coordinated safely
- Combined into a final result

This improves both performance and maintainability.

6. Latency Hiding. Many operations, especially I/O are slow. Concurrency and parallelism help hide that latency by ensuring other useful work can proceed while something is waiting.

## The Difference

Many developers use the terms **concurrency** and **parallelism** interchangeably, however they're not the same thing!

1. **Concurrency** is when we write our programs by grouping instructions into separate tasks. Some task examples could be:

- Handle HTTP requests.
- Searching for files.
- Render a frame in a videogame.

Depending on the available hardware, these tasks may or may not execute **in parallel** (for parallel execution we need multiple CPU cores). Even on a single CPU, if our code is written in a concurrent way, the system will interleave the tasks, giving the impression that is performing more that one task at a time. 

2. Parallelism requires multiple CPU cores in order to perform each task simultaneously on each core. But only **concurrent** programs can execute code in parallel. In that sense, we can say that **parallelism** is a subset of concurrency.

**Concurrency** is about planning how to do many tasks at the same time (interleaving or in parallel). **Parallelism** is about performing many tasks at the same time.

## Concurrency

**Concurrency** solves this issue having **one worker** switching between tasks so fast it looks like they are happening simultaneously.

For example, the **JavaScript runtime** is single-threaded but highly concurrent via its event loop; it interleaves tasks without parallel execution. The system starts **Task A** (e.g. sending an HTTP request), pauses it to work on **Task B** (e.g. handling a button click), then goes back to Task A. They "overlap" in timeframe, but only one is actually moving forward at any specific nanosecond.

## Parallelism

**Parallelism** solves it by having **multiple workers** doing different tasks at the exact same time.

For example, in languages like **Go**, you can spawn multiple goroutines that run truly in parallel on different CPU cores. Here, **Task A** and **Task B** can both be actively progressing at the same time, each on its own thread of execution.

<Admonition title="Both Concurrency and Parallelism">
  Many languages utilize both concepts. For example, JavaScript achieves
  **concurrency** via the *Event Loop* (handling multiple tasks on a single
  thread) but opts into **parallelism** using *Worker Threads* (executing code
  simultaneously on separate CPU cores).
</Admonition>

Go simplifies this by blending both behaviors into a single primitive: the [Goroutine](https://go.dev/tour/concurrency/1).

## Utilizing Multiple CPU Cores

Modern hardware architectures are built with [multiple cores](https://en.wikipedia.org/wiki/Multi-core_processor), so it make sense that programming languages offer a way to fully utilize the capabilities of a machine.

![multicore](../img/single-vs-multiple.webp)

Using **concurrent programming** we can split our program's workload into tasks, that can run simultaneously in multiple CPU cores. This increases the throughput of our programs, and speeds up their execution.

## Single Core Benefits

Even with a **single CPU core**, concurrency offers benefits. Most programs spend only a small proportion of their time executing computations on the processor. The majority of the time, the CPU is waiting on **slow I/O** (disk, network, user input). Instead of letting the core sit idle during those waits, concurrent programs can switch to other tasks that are ready to run, making better use of the CPU time.

<Admonition type="info" title="Real-life analogy">
  Imagine you're baking some muffins. While they are in the oven, you can be
  doing the dishes, or start another batch. Instead of sitting iddle, just
  waiting for the muffins to bake, you're using that time to get other things
  done.
</Admonition>

JavaScript is a well-known example: the language doesn’t expose primitives for splitting work across multiple CPU cores, yet its event loop and callback mechanism let it juggle many asynchronous tasks without blocking the page.
